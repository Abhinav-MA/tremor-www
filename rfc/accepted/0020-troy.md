- Feature Name: deployment_language
- Start Date: 2020-12-15
- Tremor Issue: [tremor-rs/tremor-runtime#1203](https://github.com/tremor-rs/tremor-runtime/pull/1203)
- RFC PR: [tremor-rs/tremor-rfcs#0000](https://github.com/tremor-rs/tremor-rfcs/pull/0000)

# Summary
[summary]: #summary

Add a deployment language to tremor with which whole event flows, consisting of connectors, pipelines and flows can be defined, instantiated, run and removed again.

A `connector` is an existing artifact and represents a connection between tremor and an external system.

A `pipeline` is an existing artifact and is equivalent to a `trickle` query script which compiles to an executable pipeline as defined in the `tremor-pipeline` crate.

A `flow` is a new construct which replaces the concept of a `binding` and a `mapping` in the legacy deployment syntax `yaml` which this RFC is targetted at replacing.

# Motivation
[motivation]: #motivation

Current deployments are separated into configuration scopes as follows:

1. `onramps` - these define legacy `onramp` or `source` connector configurations
2. `offramps` - these define legacy `offramp` or `sink` connector configurations
3. `pipeline` - these are referenced implicitly by name using the file stem ( name minus extension, for a top level pipeline ) passed to the tremor server command line arguments
4. `bindings` - which defines how onramps, offramps and pipelines interconnect
5. `mappings` - which instanciate and deploy binding.

The YAML format used for deployment configuration is sub-optimal for tremor application authors.

With modular tremor scripts, the business logic needed in tremor queries is significantly reduced for well-designed and well-factored tremor streaming applications. With [modular queries](../implemented/0014-modular-queries.md) entire sub-queries can be modularised and reused allowing the query logic required to compose a query application to be similarly terse.

This specification provides the same mechanisms and modularity provisions as the query and script language DSLs for deployments by replacing the YAML syntax with a modular deployment domain specific language to achieve the same hygiene factors with minimal additional syntax.

Also, given our experiences with tremor-script and tremor-query, a DSL that is well suited for the task at hand provides significant benefits:

* Helpful and consistent hygienic errors that are syntax and semantics aware
* Modularization of common definitions as reusable modules (e.g. HTTP proxy setup with sources, sinks, pipeline, and interconnecting flows)
* More expressive - the deployment language embeds the query language, which embeds the scripting language allowing more expressive application authoring.
* Paves the way for a live REPL environment in the future

The primary advantage is a capability to integrate definitions seamlessly and reuse existing language concepts and idioms. Tremor developers and operators get a consistent experience across the suite of languages and avoid distractions inherent in context switching from native DSLs to YAML which requires a higher degree of knowledge of tremor internals than desireable for new users to tremor.

# Guide-level explanation
[guide-level-explanation]: #guide-level-explanation

A useful tremor deployment always consists of one or more `connectors`, connected to one or more `pipelines`. These are deployed as a single unit that captures their interconnections and the overall flow of the deployed
application logic.

The deployment lifecycle for an artifact follows the form:

* Publish definition (with all configuration) with a unique artifact id.
* Compute a Tremor URL that uniquely identifies the instance being created.
* Incarnate or launch the instance, if not already running given its url.
* Compute the instance id and url and register the instance accordingly.
* Interconnect `connectors` and `pipelines` required for correct runtime operation.

The shutdown or undeployment sequence follows a similar form:

* Pause and disconnect the interlinked artifact instances.
* Stop and remove instances that are no longer referenced ( quiescence )
* Cascade and sweep remove any artifact instances that are no longer in use.
* Cascade and sweep remove any artifact definitions that are no longer in use.

The Tremor Deployment Language ( `troy` for short ) is the interface with
the tremor runtime and guarantees that these control interactions follow
the correct sequence of operations.

The language aims for sufficient expressivity to define complex deployment patterns, whilst providing a syntax that encourages iterative refinement and development of complex applications from basic working parts.

When a troy deployment specification is deployed - the definitions, instances and interconnections are created and launched following the lifecycle patterns summarised above by the runtime.

As with `trickle`, `troy` supports modular definition and leverages the same preprocessor `mod .. end` and `use` statements and shares the same module path and loading and module resolution mechanisms.

This allows `boilerplate` or `reusable` logic to be shared across multiple
application deployments whether deployed in the same tremor instance or otherwise.

## Example

```


# Define the `my_flow_application` deployment

define flow my_flow_application
flow
  # Defines a connector called `my_http_connector` based on
  # the builtin `http_server` connector

  define connector my_http_connector from http_server

  # Specifies required, possibly defaulted, arguments and defines
  # the public interface of the `my_http_connector` user defined
  # connector

  args
    server = "Tremor/1.2.3",

  # Defaults builtin `http_server` connector arguments, potentially
  # referencing interface arguments

  with
    codec = "json",
    preprocessors = [ "lines" ],
    config = {
      "headers": {
        "Server": args.server
      }
    }
  end;

  # Defines a connector called `my_file` based on
  # the builtin `file` connector

  define connector my_file from file

  # Has an empty interface specification

  with
      # Specializes the arguments to the builtin `file` connector
      config = {
        "path": "/snot/badger.json"
      },
      codec = "json",
      preprocessors = [ "newline" ]
  end;

  # Define a basic passthrough pipeline called `passthrough`

  define pipeline passthrough
  pipeline
      select event from in into out;
  end;
  # This flow will create one instance of `my_http_connector`

  create connector ma_http from my_http_connector
  with
    # specializing or overriding the `server` interface argument
    server = "Tremor/3.2.1"
  end;

  # This flow will create one instance of `passthrough`
  create pipeline passthrough;

  # This flow will create one instance of `my_file`
  create connector my_file;

    # Link instances by id - may reference pre-existing instances known
    # to the target runtime where this flow is deployed
    #
    connect /connector/my_http_connector to /pipeline/passthrough;

    # Alternately, may use local identifiers with explicit port assignments
    connect /pipeline/passthrough to /connector/my_file;

    # Or, local identifiers with defaulted port assignments
    connect passthrough/out to my_file/in;
end;

# Finally, instruct the runtime to `deploy` our flow application
deploy flow my_app from my_flow_application;
```

For an overview of alternatives we considered and discussed, see [Rationale and Alternatives](#rationale-and-alternatives)

# Reference-level explanation
[reference-level-explanation]: #reference-level-explanation

Troy supports two very basic operations / kinds of statements:

 * Definition of artifacts with `define` statements
 * Creation of artifact instances with `create` statements
 * Interlinking of instances with `connect` statements
 * Deployment control plane commands with `deploy` statements

## artifact definitions

Definitions of `flow` specifications are declared using the `define` statement. The BNF for which is
provided below. `connector`, `pipeline` specifications are defined inside a flow.

### Connector definitions

**EBNF grammar:**

```ebnf
connector            = "define" "connector" artifact_id "from" builtin_ref
                       [ "args" argument_list ]
                       [ with_block ]?
                       "end"
builtin_ref          = id      
artifact_id          = id     
argument_list        = required_no_default | assignment ) [ "," argument_list ]
required_no_default  = id
assignment           = id "=" expr
with_block           = "with" with_assignment_list
with_assignment_list = assignment [ "," with_assignment_list ]
```

The statement declares a user defined connector definition based on a
builtin connector provided by tremor with a user defined id. A set of
specification arguments with required and optionally defaulted parameters
can be provided in the `args` clause. A set of configuration parameters
required by the builtin connector from which the user defined connector
is being created is provided with the `with` clause.

The `args` specification is the users parameterization of the user defined
connector. It declares the intended interface to users of the connector.

The `with` specification provides required configuration to the underlying
builtin tremor connector.

Defaulted arguments provided in the `args` block can be used in the `with`
block. The values are any valid `tremor-script` expression that ordinarily
returns a value. So `match` and `patch` statements or `for` comprehensions
can be used for values, or literals. But the `emit` and `drop` expressions
which ordinarily halt event flow or `let` statements that mutate values cannot.

Space formatting characters and newlines are not significant in the BNF grammar.

**Example:**

```
define connector my_http from http_server

# Specifies the `my_http` user defined connector interface
args
    required_argument,
    optional_arg_with_default = "default value"

# Provides essential configuration to the intrinsic or builtin
# `http` connector on top of which `my_http` is defined.
with
    codec = "json",
    preprocessors = [ 
        {
            "name": "split",
            "config": {
                "split_by": "\n"
            }
        },
        "base64"
    ],
    config = {
      "host": "localhost",
      "port": args.required_argument, # TODO verify
    },
    err_required = args.optional_arg_with_default
end;
```

### Pipeline definitions

**EBNF grammar:**

```ebnf
pipeline             = "define" "pipeline" artifact_id
                       [ "args" argument_list ]
                       "pipeline" pipeline_content
                       "end"
artifact_id           = id     
argument_list         = required_no_default | assignment ) [ "," argument_list ]
required_no_default   = id
assignment            = id "=" expr
pipeline_content      = <tremor-pipeline>
```

The statement declares a user defined pipeline definition based on a user defined inline tremor query and bound to a user defined id.
A set of specification arguments with required and optionally defaulted parameters can be provided in the `args` clause.

This statement does not support a `with` clause as there are no intrinsic
artifacts being extended or parameterised by this type of definition.

The `args` specification is the users parameterization of the user defined
pipeline. It declares the intended interface to users of the pipeline.

Space formatting characters and newlines are not significant in the BNF grammar.

**Example:**

```
define pipeline my_pipeline
# Specifies the `my_http` user defined connector interface
args
    required_argument,
    optional_arg_with_default = "default value"
pipeline
    # An inline tremor query ( trickle ) script
    use std::time::nanos;
    define window fifteen_secs from tumbling
    with
        interval = nanos::from_seconds(args.required_argument),
    end;

    select { "count": aggr::stats::count(event) } from in[fifteen_secs] into out having event.count > 0;
end;
```

### Flow definitions

**EBNF grammar:**

```ebnf
connector             = "define" "flow" artifact_id
                        [ "args" argument_list ]
                        "flow" flow_stmts
                        "end"
artifact_id           = id     
argument_list         = required_no_default | assignment ) [ "," argument_list ]
required_no_default   = id
assignment            = id "=" expr
flow_stmts            = flow_stmt [ ";" flow_stmts ]*
flow_stmt             = flow_create | flow_connect | flow_define | use
use                   = "use" modular_target;
flow_define           = pipeline | connector
flow_create           = "create" create_kind instance_id "from" modular_target [ with_block ]
create_kind           = "connector" | "pipeline"
instance_id           = id
modular_target        = [ id "::" ]* id
with_block            = "with" assignment_list "end"
assignment_list       = assignment [ "," assignment_list ]
assignment            = id "=" expr
flow_connect          = "connect" connect_endpoint "to" connect_endpoint
connect_endpoint      = "/" "connector" | "pipeline" "/" id ["/" port_id]
port_id               = id
```

The statement declares a user defined flow specification. Flow specifications replace the `binding` and `mapping` constructs in the legacy YAML syntax. In
the legacy syntax the `binding` section specified how artifacts linked with each other to form a flow graph, and the `mapping` section specified how instances of the referenced artifacts were created at runtime.

Both were required to define a useful event flow graph at runtime. Production users of tremor who have been with the project from the beginning have outgrown these primitives as they are now building significant, complex, non-trivial streaming applications on top of tremor.

The YAML syntax is not modular and outside of simpler deployments the separation of instance management and interlinking doesn't work well for larger applications based on tremor.

Medium to large streaming applications were niche 18 months ago. They are normal today.

A set of specification arguments with required and optionally defaulted parameters can be provided in the `args` clause.

Flow specifications unify the creation of artifact instances and their interlinking in a hygienic fashion with potential to enhance the model in future by allowing sub flows to be defined, interlinked and instanciated in future revisions of troy.

Space formatting characters and newlines are not significant in the BNF grammar.

**Example:**

```
define flow my_eventflow
args
    required_arg,
    optional_arg = "default value"
flow
    use my_definitions;
    # Create instances required by this flow specification
    create connector my_source from my_definitions::http_connector;
    create pipeline my_pipeline from my_definitions::passthrough
    with
      required_argument = 15 # 15 second aggregation interfals
    end;
    create connector my_sink from my_definitions::my_file;

    # Interlink instances
    connect /connector/my_source/out to /pipeline/my_pipeline/in;
    connect/pipeline/my_pipeline to /connector/my_sink;
end;
```

#### Flow Create Statements

Every artifact that has a definition via `define` that is in scope in a flow
specification can be created, optionally passing parameters using a `with` clause:

**EBNF Grammar:**

```bnf
create          = "create" ("pipeline" | "connector") local_alias "from" modular_target
                  [ "with" assignment_list "end" ]
flow_id         = id
modular_target  = [ id "::" ]* id
assignment_list = assignment [ "," assignment_list ]
assignment      = id "=" expr
```

The instance_id provided in the definition coupled with the computed modular scope of the flow definition it is created within and the troy file it is created from are used to create the internal representation.

The `create` statement can be used with `connector` or `pipeline` definitions but not `flow` definitions in this iteration of the troy language.

**Example:**

```
create pipeline my_pipeline from passthrough
with
  required_argument = 15 # 15 second aggregation internals
end;
```

#### Flow Connect Statements

Flow `connect` statements allow artifact instances created in the same flow to be interlinked or artifacts that from a tremor deployment that are already deployed to be interlinked with a flow.

**EBNF Grammar:**

```bnf
flow_connect          = "connect" connect_endpoint "to" connect_endpoint
connect_endpoint      = "/" ("connector" | "pipeline") "/" id [ "/" port_id ]
port_id               = id
```

The `connect` statement links artifact instance specifications. A `from` and `to` endpoint is required. Both the `from` and `to` endpoints need to resolve to instances that are defined in the context of the flow definition they are defined within.

**Troy Native Connect Flow Form**

This uses the creation id's from the flow specification to create a reference to the target instance. If no port specification is provided then the default `in` and `out` ports are inferred automatically.

The short form:

```
connect /pipeline/passthrough to /pipeline/my_pipeline
```

Is equivalent to the longer form:

```
connect /pipeline/passthrough/out to /pipeline/my_pipeline/in
```

### Deploying Flows

Flow definitions are a specification. They define how artifacts interconnect and they specify instances to be created if required. They can also reference
pre-existing instances via their tremor URLs.

To deploy a flow into the tremor runtime and start, run and active any
connectors, pipelines and flow graph connections defined in a flow we must
use the `deploy` statement.

A unit of deployment in `troy` is a flow specification.

**EBNF Grammar:**

```bnf
create          = "deploy"  deployment_id "from" modular_target
                  [ "with" assignment_list ]
                  "end"
deployment_id   = id
modular_target  = [ id "::" ]* id
assignment_list = assignment [ "," assignment_list ]
assignment      = let_path "=" expr
let_path        = id [ "." id ]

```

The `deploy` statement references a `flow` to be deployed using
the `from` clause and may use a `with` clause to override the
default flow configuration specified for that flow, if required.


# Drawbacks
[drawbacks]: #drawbacks

* YAML is widely known especially within cloud-native environments.

* Troy is new DSL in tremor, but it builds on the same mechanisms as the query and script DSLs and leverages the same namspace and modularity mechanisms

* Introducing a new DSL steepens the learning curve for new tremor users.

# Rationale and alternatives
[rationale-and-alternatives]: #rationale-and-alternatives

- The query language `trickle` was once defined in YAML. The query language
  was designed to provide a better user experience, hygienic intuitive to interpret error reporting and a syntax and semantics that was designed to meet the needs of tremor continuous streaming query authors.

- By embedding the `tremor` script language via the query `script` operator
  the unification reduced the deployment and operational complexity and led
  to easier to develop queries.

- By following the same proven and well-trodden path with `troy`, and building on the idioms and conventions introduced with `trickle` this
should hold true for `troy`. Early indications from tremor production users is that it is a significant improvement over the YAML based deployment model.

- Lastly, removing the remaining YAML serialization, deserialization and supporting code from tremor runtime enables refactoring of complex internals that are far-reaching and affect rust-lifetimes and other performance critical or user facing parts of tremor that haven't evolved since the earliest version of tremor when YAML was introduced to tremor as a stop-gap solution at the very beginning of the project.

## Versions we considered and discarded

One initial draft contained `with` as a keyword for starting a key-value mapping (a record in tremor-script)
as a specific case only used in configuration contexts:

```
define connector artifact ws_conn
with
  type = ws,
  # nested record
  config with
    hostname = "localhost",
    port = 8080
  end,
  codec = my_json with ... end
  interceptors = ...
end;
```

This was discarded because `with` as a keyword doesn't really work as keyword for a key-value mapping. To be consistent with tremor-script, it should be `record`.

The first name for `flow` was `deployment` but it was an oversimplification. In a `flow` we specify the required artifact instances and interlinking. So `flow` is better suited and the design is richer and more expressive than the constrained syntax offered by
the YAML based `binding` and `mapping` configuration scopes.

Also, separating the runtime commands such as `deploy` from the specifications to be deployed or that are subject to lifecycle management
interactions was quickly established as a requirement when brainstorming the structure and semantics of the language and comparing it with
production use cases where the needs are very well understood.

# Prior art
[prior-art]: #prior-art

Environments like `terraform` from `HashiCorp` are a good example of resource or state management in the form of configuration as code
and the scheduler and tooling that moves a runtime from one planned state to another. In `troy` we separate out `definitions` or specifications from `flow` - which captures the runtime model and interlinking from deployment commands such as `deploy` which are commands
to the runtime to execute a change to deploye state.

Another environment worth exploring is the Erlang REPL. Erlang is unique as a language and runtime in that the REPL can be used to
navigate, explore and alter the running state of a live clustered deployemnt. In contrast to the `terraform` model, in Erlang and
other actor-based or inspired systems the runtime topology can be modified live.

Both are useful models that inspired and influenced the evolution of `troy` as documented in this RFC.

# Unresolved questions
[unresolved-questions]: #unresolved-questions

- Should we use Tremor URLs in `create` statements?
  Disposition: The simpler `connect` model is seen as sufficient at this time.

- Should we add means to refer to artifacts by their attributes (hostname, artifact-type, artifact-id, configuration , module path)?
  Disposition: There are no use cases that we know of that would benefit from this flexibility at this time.

- Should we allow users to create dormant, non-deployed and non-referenced artifacts or instances?
  Disposition: Whilst powerful, this is not seen a need at this time, and tradeoffs would need to be carefully considered.

# Future possibilities
[future-possibilities]: #future-possibilities

## Graceful Operations

We could add statement variants for create that fail if an instance with the same id already exists or that gracefully do nothing if that is the case. This graceful behavior needs to be able to verify that the existing instance is from the very same artifact, and for this might be checking the artifact content too, not merely its name.

## Define Codecs and interceptors (a.k.a pre- and post-processors)

It might be nice to be able to define codecs and interceptors as well in the deployment language.
That will mean:

* builtin codecs and interceptors are predefined in something like a Troy stdlib:

```
intrinsic codec json;
```

* codecs and interceptors can be provided with configuration and be referencable under a unique name.

```
define codec pretty_json from json
with
    pretty = true,
    indent = 4,
end;
```

This will solve the current problem that `pre-` and `postprocessors` are not configurable.
It will nonetheless introduce another type of `artifact` that actually isnt a proper artifact, and so applying the language concepts to it might not fully work out and lead to confusion.

Also it is not possible to fully define codecs and interceptors inside troy. They are all written in rust for performance reasons. The only thing we can do is to configure them and associate a codec/interceptor with its configuration and make this pair referencable within the language.

Think about what the natural extension and evolution of your proposal would
be and how it would affect Tremor as a whole in a holistic way. Try to use
this section as a tool to more fully consider all possible interactions with the
project in your proposal. Also consider how the this all fits into the roadmap for
the project and of the relevant sub-team.

This is also a good place to "dump ideas", if they are out of scope for the
RFC you are writing but otherwise related.

Note that having something written down in the future-possibilities section
is not a reason to accept the current or a future RFC; such notes should be
in the section on motivation or rationale in this or subsequent RFCs.
The section merely provides additional information.

# Errata

#### Syntax sugar for let expressions in with blocks

In order to make defining config entries in tremor-script convenient, we introduced the `with` block. We need to add a few features to tremor-script to make this work reasonably well in a configuration context, where all we want is to return a record value without too much fuss and ceremony:

* Add multi-let statements, that combine multiple `let`s inside a single statement whcih defined the variables therein and returns a record value with all those definitions.
* Add auto-creation of intermediary path segments in let stetements:

  ```
  let config.nested.value = 1;
  ```

  In this case if `config.nested` does not exist we would auto-create it as part of this let as an empty record. This statement would fail, if we would try to *nest* into an existing field that is not a record.

#### Flow Connect Statement Sugar

Connect statements describe the very basic operation needed to establish an event flow from sources/connectors via pipelines towards sinks/connectors. Defining each single connection manually might be a bit too verbose. That is why we will provide some more convenient versions that all encode `connect` statements, but are much more concise and expressive.

We call them `arrow` statements. Both the LHS and the RHS are Tremor URL string. The LHS of a an `arrow` statement is the "sender" of events, the RHS is the "receiver". The direction of the arrow describes the flow direction. If no port is provided, the LHS uses the `out` port, the RHS uses the `in` port, so users dont need to specify them in the normal case.

Examples:

```
# without ports
"/source/stdin" -> "/pipeline/pipe";
"/pipeline/pipe/err" -> "/sink/system::stderr";

# with ports
"/pipeline/pipe:err" -> "/sink/my_error_file:in";
```

`Arrow` statements also support chaining. This works as follows: As `arrow` statements when used as expression, will expose its RHS if used as LHS and its LHS if it is used as RHS. The arrow statements handle other arrow statements as LHS and RHS separately. In effect chaining `arrow` statements is equivalent to writing multiple TremorURLs connected via `arrows`:

```
"/source/system::stdin" -> "/pipeline/system::passthrough" -> "/sink/system::stderr";
```

This is equivalent to the following, when adding explicit parens to show precedence:

```
("/source/system::stdin/out" -> "/pipeline/system::passthrough/in") -> "/sink/system::stderr/in";
```

Which resolves via desugaring:

```
connect "/source/system::stdin/out" to "/pipeline/system::passthrough/in";
connect "/pipeline/system::passthrough/out" to "/sink/system::stderr/in";
```

`Arrow` statements also support tuples of Tremor URLs or tuples of other `Arrow` statements. These describe branching and joining
at the troy level.

```
"/source/system::stdin/out" -> ("/pipeline/system::passthrough", "/pipeline/my_pipe" ) -> "/sink/system::stderr/in";
```

This desugars to:

```
"/source/system::stdin/out" -> "/pipeline/system::passthrough" -> "/sink/system::stderr/in";
"/source/system::stdin/out" -> "/pipeline/my_pipe" -> "/sink/system::stderr/in";
```

If we have multiple tuples within a statement, we create statements for each combination of them. Example:

```
# full sugar
("/source/my_file", "/source/my_other_file") -> ("/pipeline/pipe1", "/pipeline/pipe2") -> "/sink/system::stderr";

# desugars to:

"/source/my_file" -> "/pipeline/pipe1" -> "/sink/system::stderr";
"/source/my_file" -> "/pipeline/pipe2" -> "/sink/system::stderr";
"/source/my_other_file" -> "/pipeline/pipe1" -> "/sink/system::stderr";
"/source/my_other_file" -> "/pipeline/pipe2" -> "/sink/system::stderr";
```

The immediate win in terseness is clear in the example.

It will be very interesting to explore how to expose the same sugar to trickle select queries. This is a future possibility.

#### Top level connect statements

Connect statements have been introduced as part of `flow` definitions. It will greatly simplify setting up tremor installations if we could write them at the top level of a troy script.

The process of describing a tremor deployment would then consist conceptually of the following steps:

1. define artifacts to be connected
2. connect those artifacts

These steps describe a good intuition about setting up a graph of nodes for events to flow through.

Example:

```
define file connector my-file-connector
with
  source = "/my-file.json",
  codec = "json"
end;

connect "/connector/my-file-connector/out" to "/pipeline/system::passthrough/in";
connect "/pipeline/system::passthrough/out" to "/connector/system::stderr/in";
```

For setting up tremor with this script, the conecpt of a `flow` doesnt event need to be introduced. Escpecially for getting started and
trying out basic configurations in a local dev environment or for tutorials and workshops, this removes friction and descreases `time-to-get-something-running-and-have-fun-understanding`.

What is going on under the hood here to make this work?

Those connect statements will be put into a synthetic `flow` artifact with the artifact id being the file in which they are declared. There is exactly one such synthetic `flow` artifact for a troy file, but only if at least 1 globale `connect` statement is given. The `flow` instance id will be `default`.
This `flow` artifact is defined and created without args upon deployment of the troy script file.

Following this route it sounds reasonable to also add `disconnect` statements as the dual of `connect`:

```
disconnect "/connector/my-file-connector/out" from "/pipeline/system::passthrough/in";
```

With `flows` we would destroy the whole `flow` instance to delete the connections therein at once. With top level `connect` statements,
we cannot reference any such instance, unless we reference it using the naming scheme above. But we would lose the power to modify single connections if we'd refrain from looking into `disconnect`.
